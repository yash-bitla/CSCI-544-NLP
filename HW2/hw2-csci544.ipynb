{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Vocabulary Creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.json') as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count occurence of every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "for train in train_data:\n",
    "    for word in train['sentence']:\n",
    "        vocab_dict[word] = 1 + vocab_dict.get(word, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "unknown_count = 0\n",
    "for key in vocab_dict.copy():\n",
    "    if vocab_dict[key] < threshold:\n",
    "        unknown_count += vocab_dict[key]\n",
    "        del vocab_dict[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort based on occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocab = sorted(vocab_dict.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    f.write('unk\\t{}\\t{}\\n'.format(index, unknown_count))\n",
    "    index += 1\n",
    "    for key, val in sorted_vocab:\n",
    "        f.write('%s\\t%s\\t%s\\n' % (key, index, val))\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Training data and vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.json', 'r') as train_file:\n",
    "    training_data = json.load(train_file)\n",
    "\n",
    "# Read vocab data from vocab.txt\n",
    "vocab_data = [line.split()[0] for line in open('vocab.txt') if line.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating transition and emission parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition, emission, label_dict = {}, {}, {}\n",
    "unique_label = set()\n",
    "\n",
    "for record in training_data:\n",
    "    labels = record['labels']\n",
    "    sentence = record['sentence']\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        word = sentence[i]\n",
    "\n",
    "        unique_label.add(label)\n",
    "        label_dict[label] = 1 + label_dict.get(label, 0)\n",
    "\n",
    "        if word.isdigit():\n",
    "            word = '<isdigit>'\n",
    "\n",
    "        if word not in vocab_data:\n",
    "            word = '<unk>'\n",
    "\n",
    "        emission[(label, word)] = 1 + emission.get((label, word), 0)\n",
    "\n",
    "        if i == 0:\n",
    "            transition[('.', label)] = 1 + transition.get(('.', label), 0)\n",
    "\n",
    "        else:\n",
    "            prev_label = labels[i-1]\n",
    "            transition[(prev_label, label)] = 1 + transition.get((prev_label, label), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_total = {}\n",
    "for t in transition:\n",
    "    label_total[t[0]] = transition[t] + label_total.get(t[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing laplace_smoothing to remove any 0 probability from the transition parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(transition_dict_counts, k, states, label_total):\n",
    "    trans_probs = {}\n",
    "    for current_state in states:\n",
    "        for next_state in states:\n",
    "            if (current_state,next_state) in transition_dict_counts:\n",
    "                tg=transition_dict_counts[current_state,next_state]\n",
    "            else:\n",
    "                tg=0\n",
    "            trans_probs[current_state, next_state] = (tg + k) / (label_total[current_state] + k * len(states))\n",
    "    return trans_probs\n",
    "\n",
    "transition = laplace_smoothing(transition, 1, list(unique_label), label_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in emission:\n",
    "    emission[i] /= label_dict[i[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating hmm.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_parameter = {}\n",
    "for key, val in transition.items():    \n",
    "    k1, k2 = key[0], key[1]\n",
    "    transition_parameter[str(k1)+\",\"+str(k2)] = val\n",
    "\n",
    "emission_parameter = {}\n",
    "for key, val in emission.items():    \n",
    "    k1, k2 = key[0], key[1]\n",
    "    emission_parameter[str(k1)+\",\"+str(k2)] = val\n",
    "\n",
    "hmm = {}\n",
    "hmm['transition'] = transition_parameter\n",
    "hmm['emission'] = emission_parameter\n",
    "\n",
    "with open('hmm.json', 'w') as json_file:\n",
    "    json.dump(hmm, json_file, indent=4, sort_keys=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many transition and emission parameters in your HMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transition parameters 2025\n",
      "Number of emission parameters 23015\n"
     ]
    }
   ],
   "source": [
    "with open('hmm.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "print(\"Number of transition parameters\", len(data['transition']))\n",
    "print(\"Number of emission parameters\", len(data['emission']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dev.json', 'r') as dev_file:\n",
    "    dev_data = json.load(dev_file)\n",
    "\n",
    "dev_words = [word for dev in dev_data for word in dev['sentence']]\n",
    "\n",
    "unknown_words = {}\n",
    "for word in train['sentence']:\n",
    "    if word not in sorted_vocab:\n",
    "        unknown_words[word] = 1 + unknown_words.get(word, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(words, unique_label):\n",
    "    res = []\n",
    "    all_state = []\n",
    "    T = list(unique_label)\n",
    "    k = 1\n",
    "\n",
    "    for key, word in enumerate(words):        \n",
    "        t = \"\"\n",
    "        max_pr = 0\n",
    "\n",
    "        if word not in vocab_data:\n",
    "            res.append((word, 'NNP'))\n",
    "            all_state.append('NNP')\n",
    "            continue\n",
    "\n",
    "        for tag in T:\n",
    "            stateval = 0\n",
    "            if key == 0:\n",
    "                trans_val = transition[('.', tag)]\n",
    "            else:\n",
    "                trans_val = transition[(all_state[-1], tag)]\n",
    "\n",
    "            for z in emission:\n",
    "                if z[1] == word and z[0] == tag:\n",
    "                    yi = trans_val*emission[z]\n",
    "                    stateval += yi\n",
    "\n",
    "            if stateval >= max_pr:\n",
    "                max_pr = stateval\n",
    "                t = tag\n",
    "                \n",
    "        all_state.append(t)\n",
    "        res.append((word, t))\n",
    "\n",
    "        k=k+1\n",
    "        if k%500==0:\n",
    "          print(k)\n",
    "\n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "49000\n",
      "49500\n",
      "50000\n",
      "50500\n",
      "51000\n",
      "51500\n",
      "52000\n",
      "52500\n",
      "53000\n",
      "53500\n",
      "54000\n",
      "54500\n",
      "55000\n",
      "55500\n",
      "56000\n",
      "56500\n",
      "57000\n",
      "57500\n",
      "58000\n",
      "58500\n",
      "59000\n",
      "59500\n",
      "60000\n",
      "60500\n",
      "61000\n",
      "61500\n",
      "62000\n",
      "62500\n",
      "63000\n",
      "63500\n",
      "64000\n",
      "64500\n",
      "65000\n",
      "65500\n",
      "66000\n",
      "66500\n",
      "67000\n",
      "67500\n",
      "68000\n",
      "68500\n",
      "69000\n",
      "69500\n",
      "70000\n",
      "70500\n",
      "71000\n",
      "71500\n",
      "72000\n",
      "72500\n",
      "73000\n",
      "73500\n",
      "74000\n",
      "74500\n",
      "75000\n",
      "75500\n",
      "76000\n",
      "76500\n",
      "77000\n",
      "77500\n",
      "78000\n",
      "78500\n",
      "79000\n",
      "79500\n",
      "80000\n",
      "80500\n",
      "81000\n",
      "81500\n",
      "82000\n",
      "82500\n",
      "83000\n",
      "83500\n",
      "84000\n",
      "84500\n",
      "85000\n",
      "85500\n",
      "86000\n",
      "86500\n",
      "87000\n",
      "87500\n",
      "88000\n",
      "88500\n",
      "89000\n",
      "89500\n",
      "90000\n",
      "90500\n",
      "91000\n",
      "91500\n",
      "92000\n",
      "92500\n",
      "93000\n",
      "93500\n",
      "94000\n",
      "94500\n",
      "95000\n",
      "95500\n",
      "96000\n",
      "96500\n",
      "97000\n",
      "97500\n",
      "98000\n",
      "98500\n",
      "99000\n",
      "99500\n",
      "100000\n",
      "100500\n",
      "101000\n",
      "101500\n",
      "102000\n",
      "102500\n",
      "103000\n",
      "103500\n",
      "104000\n",
      "104500\n",
      "105000\n",
      "105500\n",
      "106000\n",
      "106500\n",
      "107000\n",
      "107500\n",
      "108000\n",
      "108500\n",
      "109000\n",
      "109500\n",
      "110000\n",
      "110500\n",
      "111000\n",
      "111500\n",
      "112000\n",
      "112500\n",
      "113000\n",
      "113500\n",
      "114000\n",
      "114500\n",
      "115000\n",
      "115500\n",
      "116000\n",
      "116500\n",
      "117000\n",
      "117500\n",
      "118000\n",
      "118500\n",
      "119000\n",
      "119500\n",
      "120000\n",
      "120500\n",
      "121000\n",
      "121500\n",
      "122000\n",
      "122500\n",
      "123000\n",
      "123500\n",
      "124000\n"
     ]
    }
   ],
   "source": [
    "res = greedy_decoding(dev_words, unique_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Greedy Decoding using HMM: 90.72764252322263\n",
      "Error Rate: 9.272357476777373\n"
     ]
    }
   ],
   "source": [
    "ground_truth = [(word, label) for entry in dev_data for word, label in zip(entry[\"sentence\"], entry[\"labels\"])]\n",
    "\n",
    "correct_predictions = sum(1 for (wordA, labelA), (wordB, labelB) in zip(res, ground_truth) if labelA == labelB)\n",
    "total_predictions = len(ground_truth)\n",
    "\n",
    "accuracy = (correct_predictions / total_predictions) * 100\n",
    "incorrect = 100 - accuracy\n",
    "\n",
    "print(\"Accuracy for Greedy Decoding using HMM:\", accuracy)\n",
    "print(\"Error Rate:\", incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = []\n",
    "for entry in dev_data:\n",
    "    sentence = entry[\"sentence\"]\n",
    "    labels = entry[\"labels\"]\n",
    "    pairs = list(zip(sentence, labels))\n",
    "    ground_truth.extend(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "wrong_predictions = 0\n",
    "\n",
    "# Iterate through both lists and compare predictions\n",
    "for (wordA, labelA), (wordB, labelB) in zip(res, ground_truth):\n",
    "    if labelA == labelB:\n",
    "        correct_predictions += 1\n",
    "    else:\n",
    "        wrong_predictions += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct 90.72764252322263\n",
      "incorrect 9.272357476777367\n"
     ]
    }
   ],
   "source": [
    "print(\"correct\",(correct_predictions/(wrong_predictions+correct_predictions))*100)\n",
    "print(\"incorrect\",(wrong_predictions/(wrong_predictions+correct_predictions))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "49000\n",
      "49500\n",
      "50000\n",
      "50500\n",
      "51000\n",
      "51500\n",
      "52000\n",
      "52500\n",
      "53000\n",
      "53500\n",
      "54000\n",
      "54500\n",
      "55000\n",
      "55500\n",
      "56000\n",
      "56500\n",
      "57000\n",
      "57500\n",
      "58000\n",
      "58500\n",
      "59000\n",
      "59500\n",
      "60000\n",
      "60500\n",
      "61000\n",
      "61500\n",
      "62000\n",
      "62500\n",
      "63000\n",
      "63500\n",
      "64000\n",
      "64500\n",
      "65000\n",
      "65500\n",
      "66000\n",
      "66500\n",
      "67000\n",
      "67500\n",
      "68000\n",
      "68500\n",
      "69000\n",
      "69500\n",
      "70000\n",
      "70500\n",
      "71000\n",
      "71500\n",
      "72000\n",
      "72500\n",
      "73000\n",
      "73500\n",
      "74000\n",
      "74500\n",
      "75000\n",
      "75500\n",
      "76000\n",
      "76500\n",
      "77000\n",
      "77500\n",
      "78000\n",
      "78500\n",
      "79000\n",
      "79500\n",
      "80000\n",
      "80500\n",
      "81000\n",
      "81500\n",
      "82000\n",
      "82500\n",
      "83000\n",
      "83500\n",
      "84000\n",
      "84500\n",
      "85000\n",
      "85500\n",
      "86000\n",
      "86500\n",
      "87000\n",
      "87500\n",
      "88000\n",
      "88500\n",
      "89000\n",
      "89500\n",
      "90000\n",
      "90500\n",
      "91000\n",
      "91500\n",
      "92000\n",
      "92500\n",
      "93000\n",
      "93500\n",
      "94000\n",
      "94500\n",
      "95000\n",
      "95500\n",
      "96000\n",
      "96500\n",
      "97000\n",
      "97500\n",
      "98000\n",
      "98500\n",
      "99000\n",
      "99500\n",
      "100000\n",
      "100500\n",
      "101000\n",
      "101500\n",
      "102000\n",
      "102500\n",
      "103000\n",
      "103500\n",
      "104000\n",
      "104500\n",
      "105000\n",
      "105500\n",
      "106000\n",
      "106500\n",
      "107000\n",
      "107500\n",
      "108000\n",
      "108500\n",
      "109000\n",
      "109500\n",
      "110000\n",
      "110500\n",
      "111000\n",
      "111500\n",
      "112000\n",
      "112500\n",
      "113000\n",
      "113500\n",
      "114000\n",
      "114500\n",
      "115000\n",
      "115500\n",
      "116000\n",
      "116500\n",
      "117000\n",
      "117500\n",
      "118000\n",
      "118500\n",
      "119000\n",
      "119500\n",
      "120000\n",
      "120500\n",
      "121000\n",
      "121500\n",
      "122000\n",
      "122500\n",
      "123000\n"
     ]
    }
   ],
   "source": [
    "with open('data/test.json', 'r') as test_file:\n",
    "    test_data = json.load(test_file)\n",
    "\n",
    "test_words = [word for test in test_data for word in test['sentence']]\n",
    "\n",
    "test_res = greedy_decoding(test_words, unique_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating greedy.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test.json', 'r') as test_file:\n",
    "    greedy_data = json.load(test_file)\n",
    "\n",
    "k = 0\n",
    "for i in greedy_data:\n",
    "    test_labels = [label for word, label in zip(i['sentence'], test_res[k:k+len(i['sentence'])])]\n",
    "    k += len(i['sentence'])\n",
    "    i['labels'] = test_labels\n",
    "\n",
    "output_file = 'greedy.json'\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    json.dump(greedy_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dev.json', 'r') as dev_file:\n",
    "    dev_data = json.load(dev_file)\n",
    "\n",
    "dev_words = [word for dev in dev_data for word in dev['sentence']]\n",
    "\n",
    "dev_sentences = [[word for word in dev['sentence']] for dev in dev_data]\n",
    "\n",
    "dev_sentences2 = [[f\"{word}/{label}\" for word, label in zip(dev['sentence'], dev['labels'])] for dev in dev_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_SUFFIX = [\"action\", \"age\", \"ance\", \"cy\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\",\n",
    "               \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"dom\", \"ty\"]\n",
    "VB_SUFFIX = [\"ed\", \"ify\", \"ise\", \"ize\", \"ate\", \"ing\"]\n",
    "JJ_SUFFIX = [\"ous\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"able\",\"wise\"]\n",
    "ADV_SUFFIX = [\"ward\"]\n",
    "VBG_suffix=\"ing\"\n",
    "VBN_suffix=\"ed\"\n",
    "NNS_suffix=[\"s\",\"ies\",\"wards\",\"es\"]\n",
    "\n",
    "def viterbi(sentence_list, unique_label, emission, transition):\n",
    "    def get_emission_probability(tag, word):\n",
    "        return emission.get((tag, word), SMALL_PROB)\n",
    "\n",
    "    def get_transition_probability(prev_tag, current_tag):\n",
    "        return transition.get((prev_tag, current_tag), SMALL_PROB)\n",
    "\n",
    "    def handle_unknown_word(word):\n",
    "        if any(char.isdigit() for char in word):\n",
    "            if word.startswith('$'):\n",
    "                return \"CD\"\n",
    "            return \"CD\"\n",
    "        elif any(char.isupper() for char in word):\n",
    "            return \"NNP\"\n",
    "        elif any(word.endswith(suffix) for suffix in NN_SUFFIX):\n",
    "            return \"NN\"\n",
    "        elif any(word.endswith(suffix) for suffix in VB_SUFFIX):\n",
    "            return \"VB\"\n",
    "        elif any(word.endswith(suffix) for suffix in JJ_SUFFIX):\n",
    "            return \"JJ\"\n",
    "        elif any(word.endswith(suffix) for suffix in ADV_SUFFIX):\n",
    "            return \"RB\"\n",
    "        elif any(word.endswith(suffix) for suffix in NNS_suffix):\n",
    "            return \"NNS\"\n",
    "        elif word.endswith(\"ing\"):\n",
    "            return \"VBG\"\n",
    "        elif word.endswith(\"ed\"):\n",
    "            return \"VBN\"\n",
    "        elif word.istitle():\n",
    "            return \"NNP\"\n",
    "        elif word.endswith(\"'s\"):\n",
    "            return\"POS\"\n",
    "        elif '-' in word:\n",
    "            return \"JJ\"\n",
    "        return \"NNP\"\n",
    "\n",
    "    result = []\n",
    "    SMALL_PROB = 1e-10  # A small value to avoid zero probabilities\n",
    "\n",
    "    for sentences in sentence_list:\n",
    "        V = []\n",
    "        V_BP = []\n",
    "\n",
    "        for wno, word in enumerate(sentences):\n",
    "            V.append({})\n",
    "            V_BP.append({})\n",
    "\n",
    "            if wno == 0:\n",
    "                for t2 in unique_label:\n",
    "                    et = get_emission_probability(t2, word)\n",
    "                    tt = get_transition_probability('.', t2)\n",
    "                    V[wno][t2] = et * tt\n",
    "                    V_BP[wno][t2] = '.'\n",
    "\n",
    "            else:\n",
    "                for t2 in unique_label:\n",
    "                    max_prob = -math.inf\n",
    "                    best_prev_tag = None\n",
    "\n",
    "                    for t1 in V_BP[wno - 1]:\n",
    "                        et = get_emission_probability(t2, word)\n",
    "                        tt = get_transition_probability(t1, t2)\n",
    "                        curr_prob = V[wno - 1][t1] + math.log(et) + math.log(tt)\n",
    "\n",
    "                        if curr_prob > max_prob:\n",
    "                            max_prob = curr_prob\n",
    "                            best_prev_tag = t1\n",
    "\n",
    "                    V[wno][t2] = max_prob\n",
    "                    V_BP[wno][t2] = best_prev_tag\n",
    "\n",
    "                # Handle unknown words and digits\n",
    "                if all(get_emission_probability(t2, word) == SMALL_PROB for t2 in unique_label):\n",
    "                    if word.isdigit():\n",
    "                        unknown_tag = '<isdigit>'\n",
    "                    else:\n",
    "                        unknown_tag = handle_unknown_word(word)\n",
    "                    V[wno][unknown_tag] = max(V[wno].values())\n",
    "                    V_BP[wno][unknown_tag] = max(V_BP[wno].values())\n",
    "\n",
    "        best_tag = max(V[-1], key=V[-1].get)\n",
    "        tagged_sentence = [sentences[-1] + '/' + best_tag]\n",
    "\n",
    "        for i in range(len(V) - 2, -1, -1):\n",
    "            best_tag = V_BP[i + 1][best_tag]\n",
    "            tagged_sentence.append(sentences[i] + '/' + best_tag)\n",
    "\n",
    "        result.append(tagged_sentence[::-1])  # Reverse the list for the correct order\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vit=viterbi(dev_sentences, unique_label, emission, transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Viterbi Decoding using HMM: 89.98%\n",
      "Error Rate: 10.02%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_calculate(gold_standard, predicted):\n",
    "    correct = sum(1 for gold, pred in zip(gold_standard, predicted) if gold == pred)\n",
    "    total = len(gold_standard)\n",
    "    incorrect = total - correct\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy, incorrect / total if total > 0 else 0.0\n",
    "\n",
    "ground = [x.split('/') for sentence in dev_sentences2 for x in sentence]\n",
    "pred = [x.split('/') for sentence in dev_vit for x in sentence]\n",
    "\n",
    "accuracy, error_rate = accuracy_calculate(ground, pred)\n",
    "print(f\"Accuracy for Viterbi Decoding using HMM: {accuracy * 100:.2f}%\")\n",
    "print(f\"Error Rate: {error_rate * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test.json', 'r') as test_file:\n",
    "    test_data = json.load(test_file)\n",
    "\n",
    "test_sentences = [[word for word in test['sentence']] for test in test_data]\n",
    "\n",
    "test_vit=viterbi(test_sentences, unique_label, emission, transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating viterbi.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test.json', 'r') as test_file:\n",
    "    greedy_data = json.load(test_file)\n",
    "\n",
    "for index, data in enumerate(greedy_data):\n",
    "    data['labels'] = [test_vit[index][i].split('/')[1] for i in range(len(data['sentence']))]\n",
    "\n",
    "output_file = 'viterbi.json'\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    json.dump(greedy_data, outfile, indent=4)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
